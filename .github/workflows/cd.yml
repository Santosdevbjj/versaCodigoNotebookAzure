name: CD

on:
  push:
    branches: [ main ]
  workflow_dispatch:  # permite rodar manualmente

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    # 1. Checkout do código
    - name: Checkout repository
      uses: actions/checkout@v2

    # 2. Configuração do Python
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    # 3. Instalação do Databricks CLI
    - name: Install Databricks CLI
      run: |
        python -m pip install --upgrade pip
        pip install databricks-cli jq

    # 4. Configuração do Databricks CLI
    - name: Configure Databricks CLI
      run: |
        mkdir -p ~/.databricks
        cat <<EOF > ~/.databricks/config
        [DEFAULT]
        host = ${{ secrets.DATABRICKS_HOST }}
        token = ${{ secrets.DATABRICKS_TOKEN }}
        EOF

    # 5. Deploy dos notebooks e libs
    - name: Deploy notebooks and libs to Databricks
      run: |
        databricks workspace import_dir notebooks /Projects/versaCodigoNotebookAzure/notebooks --overwrite
        databricks workspace import_dir libs /Projects/versaCodigoNotebookAzure/libs --overwrite

    # 6. Deploy do job pipeline
    - name: Deploy job pipeline
      run: |
        JOB_NAME="job-pipeline-versaCodigo"
        JOB_ID=$(databricks jobs list --output JSON | jq -r ".jobs[] | select(.settings.name==\"$JOB_NAME\") | .job_id")

        if [ -z "$JOB_ID" ]; then
          echo "Job não existe, criando..."
          databricks jobs create --json-file jobs/job_pipeline.json
        else
          echo "Job já existe (ID: $JOB_ID), atualizando..."
          databricks jobs reset --job-id $JOB_ID --json-file jobs/job_pipeline.json
        fi
