### Controle e Versionamento de CÃ³digo no Notebook da Azure

![Azure_Databricks01](https://github.com/user-attachments/assets/d30d3cd1-7b30-4247-8f19-a9d9941c9c8b) 


**Bootcamp Microsoft AI for Tech - Azure Databricks** 

---

**DESCRIÃ‡ÃƒO:**

Este projeto demonstra como utilizar o Azure Databricks para versionamento e organizaÃ§Ã£o de notebooks em ambientes de dados.

A proposta inclui a criaÃ§Ã£o de clusters, importaÃ§Ã£o de arquivos, execuÃ§Ã£o de notebooks com auxÃ­lio de inteligÃªncia artificial, alÃ©m da integraÃ§Ã£o com Azure DevOps para controle de cÃ³digo e automaÃ§Ã£o de esteiras de CI/CD.

Ã‰ apresentado o uso prÃ¡tico da IA integrada ao Databricks para geraÃ§Ã£o de cÃ³digo em Python e Spark, facilitando a criaÃ§Ã£o de notebooks interativos com filtros, sumarizaÃ§Ãµes, visualizaÃ§Ãµes e comentÃ¡rios explicativos.

TambÃ©m sÃ£o exploradas boas prÃ¡ticas de organizaÃ§Ã£o, exportaÃ§Ã£o e reaproveitamento de notebooks, bem como o uso de recursos do Microsoft Learn, que oferecem exercÃ­cios guiados e roteiros de aprendizado.

A abordagem permite trabalhar de forma colaborativa, segura e com versionamento estruturado em ambientes de anÃ¡lise, engenharia de dados e machine learning dentro da plataforma Azure.


---


ğŸ“˜ **Projeto VersaCÃ³digo Notebook Azure + Databricks**

Este repositÃ³rio contÃ©m um pipeline completo de engenharia de dados utilizando Azure Databricks, PySpark, CI/CD com GitHub Actions e boas prÃ¡ticas de desenvolvimento Python (lint, testes, formataÃ§Ã£o automÃ¡tica).  

O objetivo Ã© provisionar clusters, ingerir dados em camadas (bronze, silver, gold), aplicar transformaÃ§Ãµes e anÃ¡lises, alÃ©m de garantir qualidade e automaÃ§Ã£o via testes e pipelines.

---

ğŸš€ **Tecnologias Utilizadas**

- Python 3.12  
- PySpark para processamento distribuÃ­do  
- Databricks para execuÃ§Ã£o de notebooks e jobs  
- Azure Storage para persistÃªncia de dados  
- GitHub Actions para CI/CD  
- Docker + Docker Compose para containerizaÃ§Ã£o  
- Pre-commit hooks (Black, Isort, Flake8) para padronizaÃ§Ã£o de cÃ³digo  
- Pytest para testes automatizados  

---

ğŸ’» **Requisitos de Hardware e Software**

- **Hardware mÃ­nimo:**
  - CPU: 4 cores
  - RAM: 8 GB
  - Armazenamento: 10 GB livres

- **Software:**
  - Python 3.12+
  - Docker e Docker Compose
  - Git
  - Conta no Azure Databricks configurada

---

âš™ï¸ **ConfiguraÃ§Ãµes NecessÃ¡rias**

**1. Instalar dependÃªncias de produÃ§Ã£o:**
   `bash
   pip install -r requirements.txt
   `

**2. Instalar dependÃªncias de desenvolvimento:**
   `bash
   pip install -r requirements-dev.txt
   `

**3. Configurar pre-commit hooks:**
   `bash
   pre-commit install
   `

**4. Executar localmente com Docker:**
   `bash
   docker-compose up -d
   `

**5. Rodar testes:**
   `bash
   pytest
   `

---

ğŸ“‚ **Estrutura do RepositÃ³rio**



<img width="609" height="1411" alt="Screenshot_20251115-143245" src="https://github.com/user-attachments/assets/03b247ec-efce-4555-9c18-ea187cd50dd5" />



---

ğŸ“– **ExplicaÃ§Ã£o Detalhada das Pastas e Arquivos**

ğŸ“’ **Notebooks**
- **01provisionamentocluster.py** â†’ script para provisionar cluster Databricks.  
- **02ingestaobronze.py** â†’ ingestÃ£o inicial dos dados na camada bronze.  
- **03transformacaosilver.py** â†’ transformaÃ§Ã£o e limpeza dos dados para camada silver.  
- **04analisevisualizacao.py** â†’ anÃ¡lises exploratÃ³rias e visualizaÃ§Ãµes.  
- **runner_pipeline.py** â†’ orquestra execuÃ§Ã£o sequencial dos notebooks.  

ğŸ“š **Libs**
- **io_utils.py** â†’ funÃ§Ãµes utilitÃ¡rias de leitura/escrita de dados.  
- **spark_session.py** â†’ inicializaÃ§Ã£o e configuraÃ§Ã£o da sessÃ£o Spark.  
- **validation.py** â†’ funÃ§Ãµes de validaÃ§Ã£o de dados e schemas.  
- **__init__.py** â†’ torna a pasta um pacote Python.  

âš™ï¸ **Jobs**
- **job_pipeline.json** â†’ definiÃ§Ã£o de job Databricks para rodar o pipeline.  

â˜ï¸ **Databricks**
- **pipelines/ci.yml** â†’ pipeline de integraÃ§Ã£o contÃ­nua no Databricks.  
- **pipelines/cd.yml** â†’ pipeline de entrega contÃ­nua no Databricks.  
- **workspace_export/notebooks.dbc** â†’ exportaÃ§Ã£o dos notebooks em formato Databricks.  
- **config/cluster_template.json** â†’ template de configuraÃ§Ã£o de cluster.  
- **config/job_template.json** â†’ template de configuraÃ§Ã£o de job.  

ğŸ”„ **GitHub Actions**
- **.github/workflows/ci.yml** â†’ pipeline de CI (lint, testes, build).  
- **.github/workflows/cd.yml** â†’ pipeline de CD (deploy).  

ğŸ“‘ **DocumentaÃ§Ã£o**
- **guianomenclaturaazure.md** â†’ guia de boas prÃ¡ticas de nomenclatura no Azure.  
- **imagens/** â†’ diagramas e prints do portal Azure e Databricks.  

ğŸ§ª **Testes**
- **testioutils.py** â†’ testa funÃ§Ãµes de leitura/escrita.  
- **testsparksession.py** â†’ testa inicializaÃ§Ã£o da sessÃ£o Spark.  
- **test_validation.py** â†’ testa funÃ§Ãµes de validaÃ§Ã£o.  
- **notebooks/test_pipeline.py** â†’ testa execuÃ§Ã£o do pipeline de notebooks.  
- **data/** â†’ arquivos CSV de exemplo (vendas, clientes, produtos).  

ğŸ› ï¸ **ConfiguraÃ§Ã£o e Build**
- **.gitignore** â†’ arquivos ignorados pelo Git.  
- **.flake8** â†’ configuraÃ§Ã£o de lint.  
- pyproject.toml â†’ configuraÃ§Ãµes unificadas (Black, Isort, Flake8).  
- **setup.cfg** â†’ configuraÃ§Ãµes adicionais do Flake8.  
- **requirements.txt** â†’ dependÃªncias de produÃ§Ã£o.  
- **requirements-dev.txt** â†’ dependÃªncias de desenvolvimento.  
- **Makefile** â†’ comandos automatizados (formatar, lint, testes).  
- **format.sh** â†’ script para rodar Black, Isort e Flake8.  
- **.git/hooks/pre-commit** â†’ hook local para rodar format.sh antes do commit.  
- **.pre-commit-config.yaml** â†’ configuraÃ§Ã£o do pre-commit framework.  

ğŸ³ **ContainerizaÃ§Ã£o**
- **Dockerfile** â†’ imagem otimizada de produÃ§Ã£o.  
- **docker-compose.yml** â†’ orquestraÃ§Ã£o de app + banco + cache em produÃ§Ã£o.  
- **docker-compose.override.yml** â†’ configuraÃ§Ã£o extra para desenvolvimento (hot-reload, debug).  

---

ğŸ¯ **Como Executar o Projeto**

**1. Clonar repositÃ³rio:**
   `bash
   git clone https://github.com/seu-org/seu-repo.git
   cd seu-repo
   `

**2. Instalar dependÃªncias:**
   `bash
   pip install -r requirements-dev.txt
   `

**3. Rodar pre-commit hooks:**
   `bash
   pre-commit run --all-files
   `

**4. Subir ambiente com Docker Compose:**
   `bash
   docker-compose up -d
   `

**5. Executar pipeline:**
   - Via Databricks job (jobs/job_pipeline.json).  
   - Ou localmente com:
     `bash
     python notebooks/runner_pipeline.py
     `

---

âœ… **ConclusÃ£o**

Este repositÃ³rio fornece uma soluÃ§Ã£o completa de Data Engineering com Databricks, CI/CD e boas prÃ¡ticas de desenvolvimento Python.

Ele estÃ¡ pronto para ser usado tanto em produÃ§Ã£o quanto em desenvolvimento, com suporte a testes, lint, formataÃ§Ã£o automÃ¡tica e containerizaÃ§Ã£o.

---

**Autor:**
Sergio Santos 

---
**Contato:**

[![PortfÃ³lio SÃ©rgio Santos](https://img.shields.io/badge/PortfÃ³lio-SÃ©rgio_Santos-111827?style=for-the-badge&logo=githubpages&logoColor=00eaff)](https://santosdevbjj.github.io/portfolio/)
[![LinkedIn SÃ©rgio Santos](https://img.shields.io/badge/LinkedIn-SÃ©rgio_Santos-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/in/santossergioluiz) 


---



